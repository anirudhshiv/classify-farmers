{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DecepDetect (1).ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"-8C_lrUsj12N","colab_type":"code","colab":{}},"source":["!pip install numpy --upgrade"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bEEKVzNnhFL3","colab_type":"code","colab":{}},"source":["import numpy as np\n","import os\n","import fnmatch\n","import numpy as np\n","from sklearn.utils import shuffle\n","from sklearn.preprocessing import normalize\n","import tensorflow as tf\n","from tensorflow.contrib.rnn import BasicLSTMCell, BasicRNNCell, GRUCell\n","\n","tf.flags.DEFINE_float(\"lr\", 0.001, \"learning rate\")\n","tf.flags.DEFINE_integer('epochs', 5, 'number of epoch')\n","tf.flags.DEFINE_integer(\"hidden_size\", 256, \"hidden size for each layer\")\n","tf.flags.DEFINE_integer('batch_size', 1, 'batch size')\n","tf.flags.DEFINE_integer('eval_every', 200,\n","                        'evaluation after number of train steps')\n","tf.flags.DEFINE_bool('normalize', False, 'normalize feature data')\n","tf.flags.DEFINE_float('dropout', 0.2, 'dropout rate')\n","tf.flags.DEFINE_string('model', 'RNN', 'RNN, GRU or LSTM')\n","tf.flags.DEFINE_string('data_dir', 'DecepDetect - test_run_npy/', 'directory of original data files')\n","tf.flags.DEFINE_string('log_direc', 'DecepDetect - test_run_npy/', 'directory to save log file')\n","tf.flags.DEFINE_bool('per_frame', True, 'RNN on per frame (row) data instead '\n","                                        'of taking the whole MFCC vector ')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KJcFXGcmjOE8","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","os.chdir(\"/content/gdrive/My Drive\")\n","os.getcwd()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"05iAPQzjiCxC","colab_type":"code","colab":{}},"source":["\n","\n","FLAGS = tf.app.flags.FLAGS\n","\n","tf.logging.set_verbosity(tf.logging.INFO)\n","\n","tf.app.flags.DEFINE_string('f', '', 'kernel')\n","\n","class Params(object):\n","    \"\"\" hyper-parameters \"\"\"\n","    \n","    lr = FLAGS.lr\n","    epochs = FLAGS.epochs\n","    hidden_size = FLAGS.hidden_size\n","    batch_size = FLAGS.batch_size\n","    train_steps = 0\n","    eval_steps = 0\n","    eval_every = FLAGS.eval_every\n","    normalize = FLAGS.normalize\n","    dropout = FLAGS.dropout\n","    model = FLAGS.model\n","    data_dir = FLAGS.data_dir\n","    log_dir = FLAGS.log_direc\n","    num_classes = 2\n","    feature_length = 40\n","    max_length = 0\n","    per_frame = FLAGS.per_frame\n","\n","\n","def generate_data(params):\n","    \"\"\" Extract data and transcript from FLAGS.data_dir\n","    Note: 0 indicate True, 1 indicate Lie Up, 2 indicate Lie Down for labels\n","    \"\"\"\n","    if not os.path.exists(params.data_dir):\n","        print(\"Data directory %s not found\" % params.data_dir)\n","        exit()\n","    features = []\n","    labels = []\n","    sequence_length = []\n","    for subdir, dirs, files in os.walk(params.data_dir):\n","          for i in range(len(files)):\n","              # read MFCC vector from npy file\n","              features.append(np.transpose(np.load(os.path.join(subdir, files[i]))))\n","              if files[i].startswith('T'):\n","                  labels.append(0)\n","              elif files[i].startswith('F'):\n","                  labels.append(1)\n","              else:\n","                  print(\"Incorrect label: %s\" % label)\n","                  exit()\n","\n","    # add padding to create equal length MFCC vectors\n","    params.max_length = max([feature.shape[0] for feature in features])\n","    \n","    for f in features:\n","      print (f.shape)\n","    \n","    \n","    for i in range(len(features)):\n","        # pad vectors\n","        padding = params.max_length - features[i].shape[0]\n","        sequence_length.append(features[i].shape[0])\n","        features[i] = np.vstack((features[i], np.zeros(shape=(padding, params.feature_length))))\n","\n","\n","    # convert to ndarray\n","    features, labels = np.asarray(features), np.asarray(labels)\n","\n","    # normalize features\n","    if params.normalize:\n","        shape = features.shape\n","        # normalize function only takes 2D matrix\n","        features = np.reshape(features, newshape=(shape[0], shape[1] * shape[2]))\n","        features = normalize(features, norm='l2')\n","        features = np.reshape(features, newshape=shape)\n","\n","    assert features.shape[0] == labels.shape[0] == len(sequence_length)\n","\n","    # randomly shuffle data\n","    features, labels, sequence_length = shuffle(features, labels, sequence_length, random_state=1)\n","\n","    return features, labels, sequence_length\n","\n","\n","def metric_fn(labels, predictions):\n","    \"\"\" Metric function for evaluations\"\"\"\n","    return {'eval_accuracy': tf.metrics.accuracy(labels, predictions),\n","            'eval_precision': tf.metrics.precision(labels, predictions),\n","            'eval_recall': tf.metrics.recall(labels, predictions)}\n","\n","\n","def rnn(features, mode, params):\n","    \"\"\" Recurrent model \"\"\"\n","    if params.model == \"LSTM\":\n","        cell = BasicLSTMCell(params.hidden_size)\n","    elif params.model == \"GRU\":\n","        cell = GRUCell(params.hidden_size)\n","    else:\n","        cell = BasicRNNCell(params.hidden_size)\n","\n","    initial_state = cell.zero_state(params.batch_size, dtype=tf.float64)\n","\n","    if params.per_frame:\n","        # convert input from (batch_size, max_time, ...) to\n","        # (max_time, batch_size, ...)\n","        inputs = tf.transpose(features['feature'], [1, 0, 2])\n","\n","        sequence_length = tf.reshape(\n","            features['sequence_length'],\n","            shape=(params.batch_size,)\n","        )\n","\n","        outputs, state = tf.nn.dynamic_rnn(\n","            cell,\n","            inputs=inputs,\n","            initial_state=initial_state,\n","            sequence_length=sequence_length,\n","            time_major=True\n","        )\n","\n","        # get output from the last state\n","        outputs = outputs[features['sequence_length'][0] - 1]\n","    else:\n","        # reshape MFCC vector to fit in one time step\n","        inputs = tf.reshape(\n","            features['feature'],\n","            shape=(1, params.batch_size, params.max_length * params.feature_length)\n","        )\n","\n","        outputs, state = tf.nn.dynamic_rnn(\n","            cell,\n","            inputs=inputs,\n","            initial_state=initial_state,\n","            time_major=True\n","        )\n","\n","        outputs = tf.reshape(\n","            outputs,\n","            shape=(params.batch_size, params.hidden_size)\n","        )\n","\n","    # apply dropout\n","    dropout = tf.layers.dropout(\n","        outputs,\n","        rate=params.dropout,\n","        training=mode == tf.estimator.ModeKeys.TRAIN\n","    )\n","\n","    logits = tf.layers.dense(\n","        dropout,\n","        units=params.num_classes,\n","        activation=None\n","    )\n","\n","    return logits\n","\n","\n","def model_fn(features, labels, mode, params):\n","    \"\"\" Estimator model function\"\"\"\n","    logits = rnn(features, mode, params)\n","\n","    predictions = tf.argmax(tf.nn.softmax(logits), axis=-1)\n","\n","    loss = tf.reduce_mean(\n","        tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            labels=labels,\n","            logits=logits\n","        )\n","    )\n","\n","    train_op = tf.train.AdamOptimizer(params.lr).minimize(\n","        loss=loss,\n","        global_step=tf.train.get_global_step()\n","    )\n","\n","    # metrics summary\n","    tf.summary.text('prediction', tf.as_string(predictions))\n","    tf.summary.text('label', tf.as_string(labels))\n","    accuracy = tf.metrics.accuracy(labels, predictions)\n","    tf.summary.scalar('training_accuracy', accuracy[1])\n","    precision = tf.metrics.precision(labels, predictions)\n","    tf.summary.scalar('training_precision', precision[1])\n","    recall = tf.metrics.recall(labels, predictions)\n","    tf.summary.scalar('training_recall', recall[1])\n","\n","    if mode == tf.estimator.ModeKeys.EVAL:\n","        return tf.estimator.EstimatorSpec(\n","            mode=mode,\n","            predictions=predictions,\n","            loss=loss,\n","            eval_metric_ops=metric_fn(labels, predictions)\n","        )\n","\n","    return tf.estimator.EstimatorSpec(\n","        mode=mode,\n","        predictions=predictions,\n","        loss=loss,\n","        train_op=train_op\n","    )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qhnQrCeWiLX0","colab_type":"code","colab":{}},"source":["def main():\n","    # initialize model parameters\n","    params = Params()\n","\n","    # check if log directory exist\n","    if not os.path.exists(params.log_dir):\n","        os.makedirs(params.log_dir)\n","\n","    features, labels, sequence_length = generate_data(params)\n","    # index of training and testing data split\n","    split = int(len(labels) * 0.8)\n","\n","    # calculate the amount of train and test steps\n","    params.train_steps = int(split / params.batch_size) * params.epochs\n","    params.eval_steps = int((len(features) - split) / params.batch_size)\n","\n","    def train_input_fn(params):\n","        dataset = tf.data.Dataset.from_tensor_slices((\n","            {\n","                'feature': features[:split],\n","                'sequence_length': sequence_length[:split]\n","            },\n","            labels[:split]\n","        ))\n","        dataset = dataset.repeat().batch(params.batch_size)\n","        x, y = dataset.make_one_shot_iterator().get_next()\n","        return x, y\n","\n","    def eval_input_fn(params):\n","        dataset = tf.data.Dataset.from_tensor_slices((\n","            {\n","                'feature': features[split:],\n","                'sequence_length': sequence_length[split:]\n","            },\n","            labels[split:]\n","        ))\n","        dataset = dataset.batch(params.batch_size)\n","        x, y = dataset.make_one_shot_iterator().get_next()\n","        return x, y\n","\n","    # setup Estimator configuration\n","    config = tf.estimator.RunConfig(\n","        save_checkpoints_steps=params.eval_every\n","    )\n","\n","    # define Estimator class for model\n","    estimator = tf.estimator.Estimator(\n","        model_fn=model_fn,\n","        model_dir=params.log_dir,\n","        config=config,\n","        params=params\n","    )\n","\n","    train_spec = tf.estimator.TrainSpec(\n","        input_fn=train_input_fn,\n","        max_steps=params.train_steps\n","    )\n","\n","    eval_spec = tf.estimator.EvalSpec(\n","        input_fn=eval_input_fn,\n","        steps=params.eval_steps\n","    )\n","\n","    # train and evaluate model\n","    tf.estimator.train_and_evaluate(\n","        estimator=estimator,\n","        train_spec=train_spec,\n","        eval_spec=eval_spec\n","    )\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pK-1ao3eqqWz","colab_type":"code","colab":{}},"source":["for subdir,dirs,files in os.walk(os.getcwd()):\n","  if files:\n","    if files[0].startswith('T'):\n","      print(np.load(subdir+'/'+files[3]).shape)\n","      "],"execution_count":0,"outputs":[]}]}